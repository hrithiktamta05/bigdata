Spark WordCount-----------------------------------------------------------------------------------------------------------------------------

scala> val data=sc.textFile("sparkdata.txt")  

scala> data.collect;  

scala> val splitdata = data.flatMap(line => line.split(" "));  

scala> splitdata.collect;  

scala> val mapdata = splitdata.map(word => (word,1));  

scala> mapdata.collect;  

scala> val reducedata = mapdata.reduceByKey(_+_);  

scala> reducedata.collect;  





---------------------------------------------------------------------------------------------------------------------------------------------------------





Pig WordCount------------------------------------------------------------------------------------------------------------------------------------



input2 = LOAD '/home/cloudera/wordCount.txt' AS (line:chararray);

Words = FOREACH input2 GENERATE FLATTEN(TOKENIZE(line, " "))  AS word;

Grouped = GROUP Words By word;

wordcount = FOREACH Grouped Generate COUNT(Words), group;

dump wordcount;





-----------------------------------------------------------------------------------------------------------------------------------------------------------





Spark-Shell-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------



> show dbs

admin 0.000GB

college 0.000GB

config 0.000GB

local 0.000GB

> use pranay

> db.pranay.insertMany([{id:1,name:"Pranay",gender:"male" },{id:2,name:"Giradkar",gender:"male"},{id:3,name:"Rohan",gender:"male"},{id:4,name:"Ariana",gender:"female"}])

> db.pranay.find()

> db.pranay.insertOne({id:5,name:"Ash",gender:"male"})

> db.pranay.findOne({name:"Giradkar"})

> db.pranay.find().pretty()

> db.pranay.remove({name:"Rohan"})

> db.pranay.update({name:"Ash"},{$set:{name:"Ashley"}})

> db.pranay.createIndex({"title":1})

> db.pranay.getIndexes()

> db.pranay.dropIndex({"title":9})

> db.pranay.dropIndexes()

spark-shell

val data = sc.parallelize(List(10,20,30,50))

data.collect

 :history

 1 val data = sc.parallelize(List(10,20,30,50))

 2 data.collect

 3 :history

 val mapfunc = data.map(x=>x+20)

 mapfunc.collect

res1: Array[Int] = Array(30, 40, 50, 70)

 val data = sc.parallelize(List(20,60,40,100))

 data.collect

 val filterfunc = data.filter(x=>x!20)

 val filterfunc = data.filter(x=>x!=20)

 filterfunc.collect

 val filterfunc = data.filter(x=>x=100)

 val filterfunc = data.filter(x=>x==100)

 filterfunc.collect

 val countfunc = data.count()

 countfunc.collect

 val data = sc.parallelize(List(20,60,40,100,100))

 data.collect

 val distinctfunc = data.distinct()

 distinctfunc.collect

 val data2 = sc.parallelize(List(1,2))

 data2.collect

 val data3 = sc.parallelize(List(2,3,5))

 data3.collect

 val unionfunc = data2.union(data3)

 unionfunc.collect

 val intersectfunc = data2.intersection(data3)

 intersectfunc.collect

 val cartesianfunc = data2.cartesian(data3)

 cartesianfunc.collect

 val data4 = sc.parallelize(Seq(("C",3),("B",2),("A",1),("D",4)))

 data4.collect

 val sortfunc = data4.sortBykey()

 val sortfunc = data4.sortByKey()

 sortfunc.collect

 val groupfunc = data4.groupByKey()

 groupfunc.collect

 val data4 = sc.parallelize(Seq(("C",3),("B",2),("A",1),("D",4),("B",5),("E",6)))

 data4.collect

 val sortfunc = data4.sortByKey()

 sortfunc.collect

 val reducefunc = data4.reduceByKey((value,x)=>(value+x))

 reducefunc.collect

 val data5 = sc.parallelize(Seq(("F",7),("E",6),("D",5)))

 data5.collect

 val cogroup = data4.cogroup(data5)

 cogroup.collect

 val data0 = sc.parallelize(List(10,20,30,40,50))

 val firstfunc = data0.first()

 firstfunc.collect

 val takefunc = data).take(3)

 val takefunc = data0.take(3)

 data5.collect.foreach(println)

(F,7)

(E,6)

(D,5)

---------------------------------------------------------------------------------------------------------------------------------------------------------------



Hive commands-----------------------------------------------------------------------------------------------------------------------------------------







// show databases:- To check default database provided by Hive

>	show databases;

// create database:- To create a new database

>	create database [dbname];

// if not exists:- If we want to suppress the warning generated by Hive on creating the database with the same name.

>	create database if not exists [dbname];

// with dbproperties:- It allows assigning properties with the database in the form of key-value pair

>	create database [dbname] with dbproperties

(

‘creator’=’Max’, ‘date’=’1996-05-25’

);

// describe: Used to retrieve the information associated with the database

>	describe database extended [dbname]

// create an internal table:- To create table in the database

>	create table [dbname].[tablename] (

[column_names ]

);

row format delimited fields terminated by ‘,’;

// describe table:- To see metadata or structure of the table

>	describe [dbname].[tablename];

// comments:- while creating a table, we can add the comments to the columns and can also define the table properties

// create table if not exists:- Used to create a new table using the schema of an existing table

>	create table if not exists [dbname].[table_name] like [dbname].[table_name];

Load Data:-

// loading file from local file system

load data local inpath ‘e.g. any csv or txt file’ into table employee;

// loading file from hdfs

load data inpath ‘e.g. any csv or txt file’ into table employee;

 And check If it is loaded or not.

Command to exit from hive – exit;

cat > employee.txt

1,cyrus,35000

2,keegun,50000

3,allen,40000

Enter data in input file and press enter and ctrl z

Command: cat employee.txt

Command to enter hive

hive

Alter Table:-

// Rename the table_name

// add columns

// alter column_name

// delete/replace column

Static Partitioning:-

// Use the database in which you want to create a table

//creating files to store students’ data

exit;

cat>student1.txt

1,keegun,25,khalsa,Hadoop

2,cyrus,24,vidyalankar,Hadoop

3,allen,26,ruia,Hadoop

cat>student2.txt

1,saurav,23,met,java

2,seta,25,met,java

3,geeta,22,vjti,java

// Load data with partitioning and check the result by partition course field.

Dynamic Partitioning:-

// First use the database you want and set the dynamic partition values as following:-

// Create one dummy table and load data into it

exit;

cat>student_details.txt

1,victoria,23,kms,hadoop

2,elizabeth,24,wlmt,java

3,emily,24,isvt,java

4,felicity,25,ulps,hadoop

ctrlz

// Now create one partition table and insert data of dummy table stud_demo into it.

// Retrive the data by partition

HiveQL Operators:-

The HiveQL operators facilitate to perform various arithmetic and relational operations.

// Select the database

use demo;

desc employee;

select *from employee;

1.	Arithmetic Operators:-

In Hive, the arithmetic operator accepts any numeric type

// Let's see an example to increase the salary of each employee by 50

// Let's see an example to decrease the salary of each employee by 50

// Let's see an example to find out the 10% salary of each employee

2.	Relational Operators:-

In Hive, the relational operators are generally used with clauses like Join and Having to compare the existing records.

// Let's see an example to fetch the details of the employee having salary>=40000

// Let's see an example to fetch the details of the employee having salary<40000

HiveQL Functions:-

The Hive provides various in-built functions to perform mathematical and aggregate type operations.

// Select the database in which we want to create a table, load the data into the table and fetch the loaded data by using the following command:

1.	Mathematical Functions:

// Let's see an example to fetch the square root of each employee's salary.

2.	Aggregate Functions:

In Hive, the aggregate function returns a single value resulting from computation over many rows.

// Let's see an example to fetch the maximum salary of an employee.

select max(salary) from employee;

// Let's see an example to fetch the minimum salary of an employee

select min(salary) from employee;

3.	Built-in Functions:-

// Let's see an example to fetch the name of each employee in uppercase

// Let's see an example to fetch the name of each employee in lowercase

HiveQL Group By and Having Clause

The Hive Query Language provides GROUP BY and HAVING clauses that facilitate similar functionalities as in SQL.

1.	Group By Clause:

The HQL Group By clause is used to group the data from the multiple records based on one or more column. It is generally used in conjunction with the aggregate functions (like SUM, COUNT, MIN, MAX and AVG) to perform an aggregation over each group

// Select the database in which we want to create a table, load the data into the table and fetch the loaded data by using the following command:

// Now, fetch the sum of employee salaries department wise by using the following command

select department,sum(salary) from emp group by department

2.	Having Clause:

The HQL HAVING clause is used with GROUP BY clause. Its purpose is to apply constraints on the group of data produced by GROUP BY clause. Thus, it always returns the data where the condition is TRUE

// In this example, we fetch the sum of employee's salary based on department and apply the required constraints on that sum by using HAVING clause. Let's fetch the sum of employee's salary based on department having sum >= 40000 by using the following command

select department,sum(salary) from emp group by department having sum(salary)>=40000

HiveQL Order By and Sort By Clause:-

By using HiveQL ORDER BY and SORT BY clause, we can apply sort on the column. It returns the result set either in ascending or descending order.

1.	Order By Clause:

// In HiveQL, ORDER BY clause performs a complete ordering of the query result set. Hence, the complete data is passed through a single reducer. This may take much time in the execution of large datasets. However, we can use LIMIT to minimize the sorting time. Let's see an example to arrange the data in the sorted order by using ORDER BY clause.

// Select the database in which we want to create a table, load the data into the table and fetch the loaded data by using the following command:

// Let's see an example to arrange the data in the ascending order by using ORDER BY clause

select *from emp order by salary

// Now, fetch the data in the descending order by using the following command

select * from emp order by salary desc

1.	Sort By Clause:

The HiveQL SORT BY clause is an alternative of ORDER BY clause. It orders the data within each reducer. Hence, it performs the local ordering, where each reducer's output is sorted separately. It may also give a partially ordered result.

// Let's fetch the data in the descending order by using the following command

select *from emp sort by salary desc

HiveQL Join:-

create table employee1(empid int, empname string, deptid int) row format delimited fields terminated by ‘,’;

create table dept(deptid int, department_name string) row format delimited fields terminated by ‘,’;

load data local inpath ‘/home/cloudera/join1.txt’ into table employee1;

load data local inpath ‘/home/cloudera/join2.txt’ into table dept;

select *from employee1;

1 akhila 2

2 preeti 2

3 ria 1

4 alexa 4

5 meena 3

6 heena 6

select * from dept;

1 it

2 sales

3 hr

4 marketing

5 manufacturin

set hive.auto.convert.join=false

Inner join

select e1.empname, e2.department_name from employee1 e1 join dept e2 on e1.deptid=e2.deptid;

Left outer join

select e1.empname, e2.department_name from employee1 e1 left outer join dept e2 on e1.deptid=e2.deptid;

Right outer join

select e1.empname, e2.department_name from employee1 e1 right outer join dept e2 on e1.deptid=e2.deptid;

Full outer join

select e1.empname, e2.department_name from employee1 e1 full outer join dept e2 on e1.deptid=e2.deptid

view

Create view on employee table for employee whose salary greater than 2000 and then drop that view.

create view sal as select * from employee where salary>40000.0;

select * from sal;

drop view sal;

Create index on employee table then drop that view.

index

create index empindex1 on table employee(empname) as

‘org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler’ with deferred rebuild;

show index on employee;

drop index empindex1 on employee;

show index on employee;

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------





Write a program in Map Reduce for WordCount operation.---------------------------------------------------------------------------------------------------------------------------------------------------



Step 1: Open virtual box and then start cloudera quickstart

Step 2: Open eclipse present on the cloudera desktop

Step 3: Create java project

    File->New-> Java Project

    Give project name: WordCount

    Click Next

Step 4: Add Hadoop libraries to project

    Select Libraries tab->click on Add Externa Jars

    File System->user->lib->Hadoop

    Select all library(jar) files->ok

    Again click on Add Externa Jars

   File System->user->lib->Hadoop->client

   Select all library(jar) files from client->ok->finish

Step 5: Write java code for word count

   Right click on src folder of project WordCount

   New->class

   Write class name WordCountDriver

   Click Finish

   Write the code for WordCountDriver

   Same way create classes WordMapper and SumReducer

Step 6: Export the project as jar

   Right click on project WordCount and select Export>> Java>>JAR file>>Next

 Select the export destination-Click browse-give file name wordcount.jar Click ok>>Finish>>ok

Verify the jar file

 Verify the jar file through command line open terminal give command

 ls

step 7: Move the jar file to the Hadoop file system

hdfs dfs -put wordcount.jar /user/cloudera

hdfs dfs -ls

Step 8: Create the input file for the MapReduce program

Command: cat > myInputFile.txt

Welcome to the NMITD MCA

I am persuing MCA in NMITD

Enter data in input file and press enter and ctrl z

Command: cat myInputFile.txt

Step 9: Move the input file to the Hadoop file system

hdfs dfs -put myInputFile.txt /user/cloudera

hdfs dfs -ls

hdfs dfs –ls / (here / indicates root directory of Hadoop file system(hdfs))

step 10: Run mapreduce program on Hadoop

syntax: hadoop jar jarfilename.jar classname inputfilename.txt outputfoldername

command: hadoop jar wordcount.jar WordCountDriver myInputFile.txt myOutput

step 11: view output directory

hdfs dfs –ls

hdfs dfs -ls /user/cloudera/myOutput

step 12: view the output file

hdfs dfs -cat /user/cloudera/myOutput/part-r-00000

Code

WordCountDriver.java

import org.apache.hadoop.fs.Path;

import org.apache.hadoop.io.IntWritable;

import org.apache.hadoop.io.Text;

import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;

import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import org.apache.hadoop.mapreduce.Job;

public class WordCountDirver {

 public static void main(String[] args) throws Exception {

  if (args.length != 2) {

   System.out.printf("Usage: WordCount <input dir> <output dir>\n");

   System.exit(-1);

 }

 Job job = new Job();

 job.setJarByClass(WordCountDriver.class);

 job.setJobName("Word Count");

 FileInputFormat.setInputPaths(job, new Path(args[0]));

 FileOutputFormat.setOutputPath(job, new Path(args[1]));

 job.setMapperClass(WordMapper.class);

 job.setReducerClass(SumReducer.class);

 job.setMapOutputKeyClass(Text.class);

 job.setMapOutputValueClass(IntWritable.class);

 job.setOutputKeyClass(Text.class);

 job.setOutputValueClass(IntWritable.class);

 boolean success = job.waitForCompletion(true);

 System.exit(success ? 0 : 1);

 }

}

WordMapper.java

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;

import org.apache.hadoop.io.LongWritable;

import org.apache.hadoop.io.Text;

import org.apache.hadoop.mapreduce.Mapper;

public class WordMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

@Override

  public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

 String line = value.toString();

  for (String word : line.split("\\W+")) {

   if (word.length() > 0) {

 context.write(new Text(word), new IntWritable(1));

   }

 }

 }

}

SumReducer.java

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;

import org.apache.hadoop.io.Text;

import org.apache.hadoop.mapreduce.Reducer;

public class SumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

@Override

 public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {

 int wordCount = 0;

 for (IntWritable value : values) {

 wordCount += value.get();

 }

 context.write(key, new IntWritable(wordCount));

 }

}

Alternative -------------------------------------------------------------------------------------------------------------

import java.io.IOException;

import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;

import org.apache.hadoop.fs.Path;

import org.apache.hadoop.io.IntWritable;

import org.apache.hadoop.io.Text;

import org.apache.hadoop.mapreduce.Job;

import org.apache.hadoop.mapreduce.Mapper;

import org.apache.hadoop.mapreduce.Reducer;

import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;

import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {

 public static class TokenizerMapper

  extends Mapper<Object, Text, Text, IntWritable>{

 private final static IntWritable one = new IntWritable(1);

 private Text word = new Text();

 public void map(Object key, Text value, Context context

     ) throws IOException, InterruptedException {

  StringTokenizer itr = new StringTokenizer(value.toString());

  while (itr.hasMoreTokens()) {

  word.set(itr.nextToken());

  context.write(word, one);

  }

 }

 }

 public static class IntSumReducer

  extends Reducer<Text,IntWritable,Text,IntWritable> {

 private IntWritable result = new IntWritable();

 public void reduce(Text key, Iterable<IntWritable> values,

      Context context

      ) throws IOException, InterruptedException {

  int sum = 0;

  for (IntWritable val : values) {

  sum += val.get();

  }

  result.set(sum);

  context.write(key, result);

 }

 }

 public static void main(String[] args) throws Exception {

 Configuration conf = new Configuration();

 Job job = Job.getInstance(conf, "word count");

 job.setJarByClass(WordCount.class);

 job.setMapperClass(TokenizerMapper.class);

 job.setCombinerClass(IntSumReducer.class);

 job.setReducerClass(IntSumReducer.class);

 job.setOutputKeyClass(Text.class);

 job.setOutputValueClass(IntWritable.class);

 FileInputFormat.addInputPath(job, new Path(args[0]));

 FileOutputFormat.setOutputPath(job, new Path(args[1]));

 System.exit(job.waitForCompletion(true) ? 0 : 1);

 }

}



---------------------------------------------------------------------------------------------------------------------------------------------------



Write a program in Map Reduce for Matrix Multiplication-------------------------------------------------------------------------------------------------------



Step 1: Open virtual box and then start cloudera quickstart

Step 2: Open eclipse present on the cloudera desktop

Step 3: Create java project

    File->New-> Java Project

    Give project name: MatrixMultiplication

    Click Next

Step 4: Add Hadoop libraries to project

    Select Libraries tab->click on Add Externa Jars

    File System->user->lib->Hadoop

    Select all library(jar) files->ok

    Again click on Add Externa Jars

   File System->user->lib->Hadoop->client

   Select all library(jar) files from client->ok->finish

Step 5: Write java code for matrix multiplication using mapreduce

   Right click on src folder of project MatrixMultiplication

   New->class

   Write class name MatrixMultiplicationDriver

   Click Finish

   Write the code for MatrixMultiplicationDriver

   Same way create classes MatrixMultiplicationMapper and MatrixMultiplicationReducer

Step 6: Export the project as jar

   Right click on project MatrixMultiplication and select Export>> Java>>JAR file>>Next

 Select the export destination-Click browse-give file name matrixmultiplication.jar Click ok>>Finish>>ok

Verify the jar file

 Verify the jar file through command line open terminal give command

 ls

step 7: Move the jar file to the Hadoop file system

hdfs dfs -put matrixmultiplication.jar /user/cloudera

hdfs dfs -ls

Step 8: Create the input file for the MapReduce program

First input file

Command: cat > myMMatrix.txt

Enter data in input file

M,0,0,1

M,0,1,2

M,1,0,3

M,1,1,4

and press enter and ctrl z

Command: cat myMMatrix.txt

Second input file

Command: cat > myNMatrix.txt

Enter data in input file

N,0,0,3

N,0,1,6

N,1,0,4

N,1,1,2

and press enter and ctrl z

Command: cat myNMatrix.txt

Step 9: Create the input directory and move input files into it.

hdfs dfs –mkdir /user/cloudera/matrixinput

hdfs dfs -put myMMatrix.txt /user/cloudera/matrixinput/

hdfs dfs -put myNMatrix.txt /user/cloudera/matrixinput/

hdfs dfs -ls

step 10: Run mapreduce program on Hadoop

syntax: hadoop jar jarfilename.jar classname inputfoldername outputfoldername

command: hadoop jar matrixmultiplication.jar MatrixMultiplicationDriver matrixinput matrixOutput

step 11: view output directory

hdfs dfs –ls

hdfs dfs -ls /user/cloudera/matrixOutput

step 12: view the output file

hdfs dfs -cat /user/cloudera/matrixOutput/part-r-00000

Code

MatrixMultiplicationDriver.java

import org.apache.hadoop.conf.*;

import org.apache.hadoop.fs.Path;

import org.apache.hadoop.io.*;

import org.apache.hadoop.mapreduce.*;

import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;

import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;

import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

public class MatrixMultiplicationDriver {

public static void main(String[] args) throws Exception{

// TODO Auto-generated method stub

if (args.length != 2) {

System.err.println("Usage: MatrixMultiply <in_dir> <out_dir>"); System.exit(2);

}

Configuration conf = new Configuration();

// M is an m-by-n matrix; N is an n-by-p matrix.

conf.set("m", "1000");

conf.set("n", "100");

conf.set("p", "1000");

Job job = new Job(conf, "MatrixMultiply");

job.setJarByClass(MMDriver.class);

job.setOutputKeyClass(Text.class);

job.setOutputValueClass(Text.class);

job.setMapperClass(MMMapper.class);

job.setReducerClass(MMReducer.class);

job.setInputFormatClass(TextInputFormat.class);

job.setOutputFormatClass(TextOutputFormat.class);

FileInputFormat.addInputPath(job, new Path(args[0]));

FileOutputFormat.setOutputPath(job, new Path(args[1]));

job.waitForCompletion(true);

}

}

MatrixMultiplicationMapper.java

import java.io.IOException;

import org.apache.hadoop.conf.*;

import org.apache.hadoop.io.LongWritable;

import org.apache.hadoop.io.Text;

import org.apache.hadoop.mapreduce.Mapper;

public class MatrixMultiplicationMapper extends Mapper<LongWritable, Text, Text, Text>{

@Override

public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

Configuration conf = context.getConfiguration();

int m = Integer.parseInt(conf.get("m"));

int p = Integer.parseInt(conf.get("p"));

String line = value.toString();

// (M, i, j, Mij);

String[] indicesAndValue = line.split(",");

Text outputKey = new Text();

Text outputValue = new Text();

if (indicesAndValue[0].equals("M")) {

for (int k = 0; k < p; k++) {

outputKey.set(indicesAndValue[1] + "," + k); // outputKey.set(i,k);

outputValue.set(indicesAndValue[0] + "," + indicesAndValue[2]

+ "," + indicesAndValue[3]); // outputValue.set(M,j,Mij);

context.write(outputKey, outputValue);

}

} else {

// (N, j, k, Njk);

for (int i = 0; i < m; i++) {

outputKey.set(i + "," + indicesAndValue[2]);

outputValue.set("N," + indicesAndValue[1] + ","

+ indicesAndValue[3]);

context.write(outputKey, outputValue);

}

}

}

}

MatrixMultiplicationReducer.java

import org.apache.hadoop.io.Text;

import java.io.IOException;

import java.util.HashMap;

public class MatrixMultiplicationReducer extends org.apache.hadoop.mapreduce.Reducer<Text, Text, Text, Text>{

@Override

public void reduce(Text key, Iterable<Text> values, Context context)

throws IOException, InterruptedException {

String[] value;

//key=(i,k),

//Values = [(M/N,j,V/W),..]

HashMap<Integer, Float> hashA = new HashMap<Integer, Float>();

HashMap<Integer, Float> hashB = new HashMap<Integer, Float>();

for (Text val : values) {

value = val.toString().split(",");

if (value[0].equals("M")) {

hashA.put(Integer.parseInt(value[1]), Float.parseFloat(value[2]));

} else {

hashB.put(Integer.parseInt(value[1]), Float.parseFloat(value[2]));

}

}

int n = Integer.parseInt(context.getConfiguration().get("n"));

float result = 0.0f;

float m_ij;

float n_jk;

for (int j = 0; j < n; j++) {

m_ij = hashA.containsKey(j) ? hashA.get(j) : 0.0f;

n_jk = hashB.containsKey(j) ? hashB.get(j) : 0.0f;

result += m_ij * n_jk;

}

if (result != 0.0f) {

context.write(null,

new Text(key.toString() + "," + Float.toString(result)));

}

}

}



-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Sorting:------------------------------------------------------------------------------------------------------------------------------------------------------------------------







TestDriver.java

import org.apache.hadoop.fs.Path;

import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text;

import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;

import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.mapreduce.Job;

public class TestDriver {

public static void main(String[] args) throws Exception { if (args.length != 2) {

System.out.printf("Usage: WordCount <input dir>

<output dir>\n");

System.exit(-1);

}

Job job = new Job();

job.setJarByClass(testdriver.class); job.setJobName("Word Count");

FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1]));

job.setMapperClass(testmap.class);

job.setReducerClass(testreduce.class);

job.setMapOutputKeyClass(IntWritable.class);

job.setMapOutputValueClass(IntWritable.class);

job.setOutputKeyClass(IntWritable.class);

job.setOutputValueClass(IntWritable.class);

boolean success = job.waitForCompletion(true); System.exit(success ? 0 : 1);

}

}

TestMap.java

import java.io.IOException;

import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text;

import org.apache.hadoop.mapreduce.Mapper;

public class TestMap extends Mapper<LongWritable, Text, IntWritable, IntWritable> {

@Override

public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

String line = value.toString();

String[] tokens = line.split(","); // This is the delimiter between

int keypart = Integer.parseInt(tokens[0]);

int valuePart = Integer.parseInt(tokens[1]); context.write(new IntWritable(valuePart), new

IntWritable(keypart));

}

}

TestReduce.java

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;

import org.apache.hadoop.mapreduce.Reducer;

public class TestReduce extends Reducer<IntWritable, IntWritable, IntWritable, IntWritable> {

@Override

public void reduce(IntWritable key, Iterable<IntWritable> values, Context context)

throws IOException, InterruptedException {

for (IntWritable value : values) { context.write(value, key);

}

}

}

Command:

hadoop jar SortingAmazonOrders.jar testdriver amazon-orders.txt ResultSortingAmazonOrders



-----------------------------------------------------------------------------------------------------------------------------------------------------------------------





MongoDB---------------------------------------------------------------------------------------------------------------------------------------------------------



• Start MongoDB Shell: Open a terminal or command prompt and run the MongoDB shell by typing mongo. Make sure your MongoDB server is running.

• Create a Database: In the MongoDB shell, you can create a new database using the use command. For example, let's create a database named sampleDB:

 use sampleDB

This command switches to the sampleDB database. If the database doesn't exist, MongoDB will create it.

• Create a Collection: Now, let's create a collection within the sampleDB database. A collection is similar to a table in relational databases:

 db.createCollection("users")

This command creates a collection named users within the sampleDB database.

• Insert Documents: Now, let's insert some sample documents into the users collection:

db.users.insert([

  { name: "John Doe", age: 30, email: "john@example.com" },

  { name: "Jane Smith", age: 25, email: "jane@example.com" },

  { name: "Bob Johnson", age: 35, email: "bob@example.com" }

])

This command inserts three documents into the users collection.

• Query Data: You can query the data to retrieve information from the database:

db.users.find()

This command retrieves all documents from the users collection.

Indexing:

Indexing in MongoDB is crucial for optimizing query performance. Indexes can be created on one or more fields of a collection to improve the speed of data retrieval. MongoDB automatically creates an index on the _id field by default.

Creating an Index:

To create an index on a specific field, you can use the createIndex method. For example, let's create an index on the email field in the users collection:

db.users.createIndex({ email: 1 })

In this example, 1 specifies ascending order. You can use -1 for descending order.

Listing Indexes:

To view the existing indexes on a collection, you can use the getIndexes method:

db.users.getIndexes()

Delete Operations:

MongoDB provides the deleteOne and deleteMany methods to remove documents from a collection.

Deleting a Single Document:

To delete a single document based on a specific condition, you can use the deleteOne method. For example, let's delete the document where the name is "John Doe":

db.users.deleteOne({ name: "John Doe" })

Deleting Multiple Documents:

To delete multiple documents based on a specific condition, you can use the deleteMany method. For instance, let's delete all documents where the age is greater than 30:

javascript

db.users.deleteMany({ age: { $gt: 30 } })

In this example, $gt is the greater than operator.





---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------